{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from picamera2 import Picamera2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tflite_runtime.interpreter import Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0:53:06.217228782] [5365] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:284 \u001b[0mlibcamera v0.1.0+118-563cd78e\n",
      "[0:53:06.263018647] [5380] \u001b[1;33m WARN \u001b[1;37mRPiSdn \u001b[1;34msdn.cpp:39 \u001b[0mUsing legacy SDN tuning - please consider moving SDN inside rpi.denoise\n",
      "[0:53:06.265797243] [5380] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:444 \u001b[0mRegistered camera /base/soc/i2c0mux/i2c@1/ov5647@36 to Unicam device /dev/media1 and ISP device /dev/media0\n",
      "[0:53:06.265860039] [5380] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpipeline_base.cpp:1142 \u001b[0mUsing configuration file '/usr/share/libcamera/pipeline/rpi/vc4/rpi_apps.yaml'\n",
      "[0:53:06.270216794] [5365] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:284 \u001b[0mlibcamera v0.1.0+118-563cd78e\n",
      "[0:53:06.313781116] [5383] \u001b[1;33m WARN \u001b[1;37mRPiSdn \u001b[1;34msdn.cpp:39 \u001b[0mUsing legacy SDN tuning - please consider moving SDN inside rpi.denoise\n",
      "[0:53:06.316379102] [5383] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:444 \u001b[0mRegistered camera /base/soc/i2c0mux/i2c@1/ov5647@36 to Unicam device /dev/media1 and ISP device /dev/media0\n",
      "[0:53:06.316441212] [5383] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpipeline_base.cpp:1142 \u001b[0mUsing configuration file '/usr/share/libcamera/pipeline/rpi/vc4/rpi_apps.yaml'\n"
     ]
    }
   ],
   "source": [
    "picam2 = Picamera2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0:53:07.136295444] [5365] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1183 \u001b[0mconfiguring streams: (0) 160x100-XBGR8888 (1) 160x100-YUV420 (2) 1920x1080-SGBRG10_CSI2P\n",
      "[0:53:07.136825868] [5383] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:608 \u001b[0mSensor: /base/soc/i2c0mux/i2c@1/ov5647@36 - Selected sensor format: 1920x1080-SGBRG10_1X10 - Selected unicam format: 1920x1080-pGAA\n"
     ]
    }
   ],
   "source": [
    "res = (160, 100)\n",
    "preview_config = picam2.create_preview_configuration(main={\"size\": res}, lores={\"size\": res, \"format\": \"YUV420\"})\n",
    "picam2.configure(preview_config)\n",
    "stride = picam2.stream_configuration(\"lores\")[\"stride\"]\n",
    "picam2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = picam2.capture_buffer(\"lores\")\n",
    "grey = buffer[:stride*res[1]].reshape((res[1], stride))[:, :res[0]]\n",
    "input_data = np.expand_dims(grey, axis=0)\n",
    "input_data = np.expand_dims(input_data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_path = '../model_export/q-int8_d-2bit-octree.tflite'\n",
    "interpreter = Interpreter(tf_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "input_tensor_index = input_details[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (input_scale, input_zero_point) != (0.0, 0):\n",
    "\tinput_data = np.multiply(input_data, 1.0 / input_scale) + input_zero_point\n",
    "input_data = input_data.astype(input_details[\"dtype\"])\n",
    "interpreter.set_tensor(0, input_data)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[\"index\"])\n",
    "pred = output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "picam2.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
